import os, tempfile, json, re
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker
from google import genai

# ---------------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
# ---------------------------------
VECTORSTORE_PATH = "./vector_store"
os.makedirs(VECTORSTORE_PATH, exist_ok=True)

EMBEDDING_MODEL = "Omartificial-Intelligence-Space/GATE-AraBert-v1"
RERANKER_MODEL = "Omartificial-Intelligence-Space/ARA-Reranker-V1"

# Load Gemini API key from environment
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY environment variable is required")

client = genai.Client(api_key=GEMINI_API_KEY)

embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
reranker_model = HuggingFaceCrossEncoder(model_name=RERANKER_MODEL)
compressor = CrossEncoderReranker(model=reranker_model, top_n=5)

# ---------------------------------
# Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡
# ---------------------------------
async def process_upload(file):
    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¤Ù‚ØªØ§Ù‹
    with tempfile.NamedTemporaryFile(delete=False, suffix=".json") as tmp:
        tmp.write(await file.read())
        tmp_path = tmp.name

    # Ù‚Ø±Ø§Ø¡Ø© ÙˆØªØ­Ù„ÙŠÙ„ JSON
    with open(tmp_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
    os.unlink(tmp_path)

    # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ù…Ù„Ù
    if "law_sources" not in data:
        raise ValueError("âŒ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± ØµØ­ÙŠØ­ - Ø§Ù„Ù…ÙØªØ§Ø­ 'law_sources' Ù…ÙÙ‚ÙˆØ¯")

    law_source = data["law_sources"]
    
    # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    if "articles" not in law_source:
        raise ValueError("âŒ Ø§Ù„Ù…ÙØªØ§Ø­ 'articles' Ù…ÙÙ‚ÙˆØ¯ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")

    # âœ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø¥Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    documents = []
    for article in law_source["articles"]:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        if not article.get("text") or not article.get("article"):
            continue  # ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©
            
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³ØªÙ†Ø¯ Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§Ù…Ù„Ø©
        document = Document(
            page_content=article["text"],
            metadata={
                "article": article["article"],
                "keywords": article.get("keywords", []),
                "order_index": article.get("order_index", 0),
                "law_name": law_source.get("name", ""),
                "law_type": law_source.get("type", ""),
                "jurisdiction": law_source.get("jurisdiction", ""),
                "issuing_authority": law_source.get("issuing_authority", ""),
                "issue_date": law_source.get("issue_date", "")
            }
        )
        documents.append(document)

    if not documents:
        raise ValueError("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù‚Ø§Ù„Ø§Øª ØµØ§Ù„Ø­Ø© ÙÙŠ Ø§Ù„Ù…Ù„Ù")

    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ chunks
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=400, 
        chunk_overlap=50,
        separators=["\n\n", "\n", ". ", "! ", "? ", "Ø› ", "ØŒ "]  # ÙÙˆØ§ØµÙ„ Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
    )
    chunks = splitter.split_documents(documents)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ùˆ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
    if os.path.exists(f"{VECTORSTORE_PATH}/index.faiss"):
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØªØ®Ø²ÙŠÙ† Ù…ÙˆØ¬ÙˆØ¯ØŒ Ù†Ø­Ø¯Ø«Ù‡
        existing_vectorstore = FAISS.load_local(
            VECTORSTORE_PATH, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        existing_vectorstore.add_documents(chunks)
        existing_vectorstore.save_local(VECTORSTORE_PATH)
        chunks_count = len(chunks)
    else:
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯ØŒ Ù†Ù†Ø´Ø¦ Ø¬Ø¯ÙŠØ¯
        vectorstore = FAISS.from_documents(chunks, embeddings)
        vectorstore.save_local(VECTORSTORE_PATH)
        chunks_count = len(chunks)

    return chunks_count
# ---------------------------------
# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± / Ø§Ù„Ø³Ø¤Ø§Ù„
# ---------------------------------
async def answer_query(query: str):
    if not os.path.exists(f"{VECTORSTORE_PATH}/index.faiss"):
        return "âŒ Ù„Ù… ÙŠØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø¨Ø¹Ø¯."

    # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
    vectorstore = FAISS.load_local(VECTORSTORE_PATH, embeddings, allow_dangerous_deserialization=True)

    # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
    base_docs = vectorstore.similarity_search(query, k=20)
    
    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    reranked_docs = compressor.compress_documents(base_docs, query)

    # âœ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
    context_parts = []
    for doc in reranked_docs:
        metadata = doc.metadata
        context_part = f"""
ğŸ“œ **{metadata.get('article', '')}** - {metadata.get('law_name', '')}
ğŸ“ Ø§Ù„Ø¬Ù‡Ø©: {metadata.get('issuing_authority', '')}
ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±: {metadata.get('issue_date', '')}

Ø§Ù„Ù…Ø­ØªÙˆÙ‰:
{doc.page_content}

ğŸ”‘ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {', '.join(metadata.get('keywords', []))}
        """
        context_parts.append(context_part.strip())

    context_text = "\n\n" + "="*50 + "\n\n".join(context_parts) + "\n" + "="*50

    # âœ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù€ Prompt Ù„ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©
    prompt = f"""
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬Ø²Ø§Ø¦ÙŠ Ù„Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„ØªØ²ÙˆÙŠØ±.

**Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:**
1. Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ÙÙ‚Ø·
2. Ø§Ø°ÙƒØ± Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© ÙˆØ§Ù„Ù†Øµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ø¯Ù‚Ø©
3. Ø£Ø¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø© ÙˆØ§Ù„Ø¬Ù‡Ø© Ø§Ù„Ù…ØµØ¯Ø±Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ù…ØªÙˆÙØ±Ø©
4. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰
5. ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙˆØ§Ø¶Ø­Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©

**Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©:**
{context_text}

**Ø§Ù„Ø³Ø¤Ø§Ù„:**
{query}

**Ù…Ù„Ø§Ø­Ø¸Ø©:** Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø£Ø¹Ù„Ø§Ù‡ØŒ Ù‚Ù„: "Ù„Ù… Ø£Ø¬Ø¯ Ù†ØµØ§Ù‹ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ§Ù‹ ÙŠØºØ·ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø´ÙƒÙ„ Ù…Ø­Ø¯Ø¯"
"""

    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config={
                "temperature": 0.1,  # Ø£Ù‚Ù„ Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ù‚Ø©
                "max_output_tokens": 2000,
                "top_p": 0.8
            }
        )
        return response.text
    except Exception as e:
        return f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Gemini: {e}"