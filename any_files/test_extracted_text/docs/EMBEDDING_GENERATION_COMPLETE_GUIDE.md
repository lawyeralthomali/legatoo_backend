# ğŸ¤– Complete Guide: How Embeddings Are Generated

**Created**: October 9, 2025  
**Purpose**: Complete explanation of embedding generation process, scripts, models, and code

---

## ğŸ“‹ Table of Contents

1. [What Are Embeddings?](#what-are-embeddings)
2. [Which Scripts Generate Embeddings?](#which-scripts-generate-embeddings)
3. [What Models Are Used?](#what-models-are-used)
4. [How Embeddings Are Generated](#how-embeddings-are-generated)
5. [Code Walkthrough](#code-walkthrough)
6. [API Endpoints](#api-endpoints)
7. [Usage Examples](#usage-examples)

---

## ğŸ¯ What Are Embeddings?

Embeddings are **numerical representations** of text that capture semantic meaning.

### Simple Explanation:
```
Text: "ÙØ³Ø® Ø¹Ù‚Ø¯ Ø§Ù„Ø¹Ù…Ù„" (termination of employment contract)
  â†“ AI Model Converts
Embedding: [0.123, -0.456, 0.789, ..., 0.234]
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  768 numbers

These numbers capture the MEANING of the text!
Similar meanings = Similar numbers
```

### Why Do We Need Them?

1. **Semantic Search**: Find documents by meaning, not just keywords
2. **Fast Comparison**: Compare documents in milliseconds
3. **AI Understanding**: Enable AI to "understand" text content

---

## ğŸ› ï¸ Which Scripts Generate Embeddings?

### Main Scripts:

| Script | Purpose | Usage |
|--------|---------|-------|
| **`scripts/generate_embeddings_batch.py`** | Generate embeddings for all documents | Main production script |
| **`scripts/regenerate_embeddings.py`** | Regenerate embeddings with new model | Update existing embeddings |
| **API Endpoint** | Generate embeddings via API | Runtime generation |

### 1. Main Batch Generator Script

**File**: `scripts/generate_embeddings_batch.py`

**Purpose**: Process all documents and generate embeddings in batch

**Usage**:
```bash
# Process all documents
python scripts/generate_embeddings_batch.py --all

# Process only chunks without embeddings (recommended)
python scripts/generate_embeddings_batch.py --pending

# Process specific document
python scripts/generate_embeddings_batch.py --document-id 5

# Check system status
python scripts/generate_embeddings_batch.py --status

# Use specific model
python scripts/generate_embeddings_batch.py --pending --model large

# Custom batch size
python scripts/generate_embeddings_batch.py --pending --batch-size 64
```

**What It Does**:
```
1. Connects to database
2. Finds chunks without embeddings
3. Initializes AI model
4. Processes chunks in batches (32-64 at a time)
5. Generates embeddings using AI
6. Saves embeddings to database
7. Reports statistics
```

---

### 2. Regenerate Embeddings Script

**File**: `scripts/regenerate_embeddings.py`

**Purpose**: Regenerate all embeddings with updated model or updated content

**Usage**:
```bash
python scripts/regenerate_embeddings.py
```

**What It Does**:
```
1. Finds all chunks without embeddings
2. Initializes Arabic BERT model
3. Generates embeddings in batches
4. Verifies specific chunks
5. Tests search functionality
```

**When to Use**:
- After updating chunk content
- After switching to new AI model
- After fixing data issues

---

## ğŸ¤– What Models Are Used?

### Current Production Model:

**Model Name**: `paraphrase-multilingual-mpnet-base-v2`

**Details**:
- **Type**: Sentence Transformer (BERT-based)
- **Provider**: Hugging Face
- **Size**: 278M parameters
- **Embedding Dimension**: 768 floats
- **Languages**: 50+ including Arabic
- **Speed**: 50-100ms per text (CPU), 10-20ms (GPU)
- **Max Sequence**: 512 tokens (~2048 characters)

### Alternative Models Supported:

#### 1. **Arabic BERT** (`arabert`)
```python
model_name = 'arabert'
# Specialized for Arabic text
# Better for Arabic legal documents
```

#### 2. **LaBSE** (`labse`)
```python
model_name = 'labse'
# Language-agnostic BERT
# Good for multilingual content
```

#### 3. **OpenAI** (Optional)
```python
# Uses OpenAI API
model = 'text-embedding-3-large'
dimension = 3072
# Higher quality but costs money
```

### Model Comparison:

| Model | Dimension | Speed | Arabic Quality | Cost |
|-------|-----------|-------|----------------|------|
| **paraphrase-multilingual** | 768 | Fast | Good | Free âœ… |
| **arabert** | 768 | Fast | Excellent | Free âœ… |
| **labse** | 768 | Fast | Very Good | Free âœ… |
| **OpenAI text-embedding-3** | 3072 | Slow | Excellent | Paid ğŸ’° |

---

## ğŸ”„ How Embeddings Are Generated

### Complete Workflow:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EMBEDDING GENERATION FLOW                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: Document Upload
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User uploads PDF     â”‚
â”‚ "Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„.pdf"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
STEP 2: Parse & Extract
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PDF â†’ Extract text                â”‚
â”‚ Split into chunks                 â”‚
â”‚ Create KnowledgeChunk records     â”‚
â”‚                                   â”‚
â”‚ Chunk 1: "Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰..."      â”‚
â”‚ Chunk 2: "Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©..."     â”‚
â”‚ ...                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ embedding_vector = NULL (initially)
           â”‚
           â–¼
STEP 3: Run Embedding Generation Script
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ python scripts/generate_embeddings_batch.py --pending    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
STEP 4: Find Chunks Without Embeddings
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SELECT * FROM knowledge_chunk        â”‚
â”‚ WHERE embedding_vector IS NULL       â”‚
â”‚                                      â”‚
â”‚ Result: 600 chunks found             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
STEP 5: Initialize AI Model
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load paraphrase-multilingual-mpnet-base-v2      â”‚
â”‚ Model size: 278M parameters                      â”‚
â”‚ Device: GPU (if available) or CPU               â”‚
â”‚ Embedding dimension: 768                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
STEP 6: Process in Batches
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch 1: Chunks 1-32                                     â”‚
â”‚   â”œâ”€ Text 1: "Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: ÙŠØ³Ù…Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù…..."        â”‚
â”‚   â”œâ”€ Text 2: "Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: ÙŠÙ‚ØµØ¯ Ø¨Ø§Ù„Ø£Ù„ÙØ§Ø¸..."        â”‚
â”‚   â””â”€ ... (30 more)                                       â”‚
â”‚                                                          â”‚
â”‚   â†“ Feed to AI Model                                    â”‚
â”‚                                                          â”‚
â”‚   AI Model Processing:                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚ 1. Tokenization                        â”‚            â”‚
â”‚   â”‚    "Ø§Ù„Ù…Ø§Ø¯Ø©" â†’ [token_ids]              â”‚            â”‚
â”‚   â”‚                                         â”‚            â”‚
â”‚   â”‚ 2. BERT Encoding (12 layers)           â”‚            â”‚
â”‚   â”‚    [token_ids] â†’ hidden states         â”‚            â”‚
â”‚   â”‚                                         â”‚            â”‚
â”‚   â”‚ 3. Mean Pooling                        â”‚            â”‚
â”‚   â”‚    Average across tokens               â”‚            â”‚
â”‚   â”‚                                         â”‚            â”‚
â”‚   â”‚ 4. Output: [768 floats]                â”‚            â”‚
â”‚   â”‚    [0.123, -0.456, ..., 0.234]         â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                          â”‚
â”‚   â†“ Save to Database                                    â”‚
â”‚                                                          â”‚
â”‚ UPDATE knowledge_chunk                                   â”‚
â”‚ SET embedding_vector = '[0.123, -0.456, ..., 0.234]'    â”‚
â”‚ WHERE id IN (1, 2, 3, ..., 32)                          â”‚
â”‚                                                          â”‚
â”‚ âœ… Batch 1 Complete (32 chunks)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ Repeat for all batches
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch 2: Chunks 33-64   âœ…          â”‚
â”‚ Batch 3: Chunks 65-96   âœ…          â”‚
â”‚ ...                                  â”‚
â”‚ Batch 19: Chunks 577-600 âœ…         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
STEP 7: Final Report
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… PROCESSING COMPLETE                â”‚
â”‚                                      â”‚
â”‚ Total Chunks: 600                    â”‚
â”‚ Processed: 595                       â”‚
â”‚ Failed: 5                            â”‚
â”‚ Success Rate: 99.2%                  â”‚
â”‚ Time: 45.3 seconds                   â”‚
â”‚ Speed: 13.1 chunks/sec               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Code Walkthrough

### Service Layer: `ArabicLegalEmbeddingService`

**File**: `app/services/arabic_legal_embedding_service.py`

#### 1. Initialize the Service

```python
class ArabicLegalEmbeddingService:
    """Arabic Legal Embedding Service"""
    
    def __init__(
        self, 
        db: AsyncSession, 
        model_name: str = 'paraphrase-multilingual',
        use_faiss: bool = True
    ):
        self.db = db
        self.model_name = model_name
        self.use_faiss = use_faiss
        
        # Model components
        self.sentence_transformer = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # Performance settings
        self.batch_size = 64  # Process 64 chunks at once
        self.max_seq_length = 512  # Max tokens per text
        
        # Cache for speed
        self._embedding_cache = {}
```

#### 2. Load the AI Model

```python
def initialize_model(self) -> None:
    """Load the AI model"""
    model_path = self.MODELS.get(self.model_name)
    # e.g., 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
    
    logger.info(f"ğŸ“¥ Loading model: {model_path}")
    logger.info(f"ğŸ“± Device: {self.device}")
    
    # Load SentenceTransformer
    self.sentence_transformer = SentenceTransformer(model_path, device=self.device)
    
    # Get embedding dimension
    test_embedding = self.sentence_transformer.encode("test")
    self.embedding_dimension = len(test_embedding)  # 768
    
    logger.info(f"âœ… Model loaded successfully")
    logger.info(f"   Embedding dimension: {self.embedding_dimension}")
```

#### 3. Encode Text to Embedding

```python
def encode_text(self, text: str) -> np.ndarray:
    """Convert text to embedding vector"""
    
    # 1. Check cache first
    if text in self._embedding_cache:
        return self._embedding_cache[text]
    
    # 2. Ensure model is loaded
    self._ensure_model_loaded()
    
    # 3. Truncate if too long
    text = self._truncate_text(text, max_tokens=512)
    
    # 4. Use AI model to encode
    embedding = self.sentence_transformer.encode(
        text, 
        convert_to_numpy=True,
        show_progress_bar=False
    )
    # Output: numpy array of 768 floats
    
    # 5. Cache and return
    self._embedding_cache[text] = embedding
    return embedding
```

#### 4. Generate Batch Embeddings

```python
async def generate_batch_embeddings(
    self,
    chunk_ids: List[int],
    overwrite: bool = False
) -> Dict[str, Any]:
    """Generate embeddings for multiple chunks (optimized)"""
    
    # 1. Get chunks from database
    query = select(KnowledgeChunk).where(
        KnowledgeChunk.id.in_(chunk_ids)
    )
    
    if not overwrite:
        # Only chunks without embeddings
        query = query.where(
            KnowledgeChunk.embedding_vector.is_(None)
        )
    
    result = await self.db.execute(query)
    chunks = result.scalars().all()
    
    logger.info(f"ğŸ“Š Found {len(chunks)} chunks to process")
    
    # 2. Process in batches (64 at a time)
    processed = 0
    failed = 0
    
    for i in range(0, len(chunks), self.batch_size):
        batch = chunks[i:i + self.batch_size]
        logger.info(f"âš™ï¸  Processing batch {i // self.batch_size + 1}")
        
        # 3. Extract texts from chunks
        texts = [chunk.content for chunk in batch]
        
        # 4. Generate embeddings for entire batch
        embeddings = self._encode_batch(texts)
        # Uses AI model.encode() which is optimized for batches
        
        # 5. Save to database
        for chunk, embedding in zip(batch, embeddings):
            try:
                # Convert numpy array to JSON string
                chunk.embedding_vector = json.dumps(embedding.tolist())
                processed += 1
            except Exception as e:
                logger.error(f"âŒ Failed chunk {chunk.id}: {str(e)}")
                failed += 1
        
        # 6. Commit batch to database
        await self.db.commit()
    
    # 7. Return statistics
    return {
        "success": True,
        "total_chunks": len(chunks),
        "processed_chunks": processed,
        "failed_chunks": failed,
        "model": self.model_name
    }
```

#### 5. Batch Encoding (Core AI Processing)

```python
def _encode_batch(self, texts: List[str]) -> List[np.ndarray]:
    """Encode multiple texts at once (faster than one-by-one)"""
    
    self._ensure_model_loaded()
    
    # Use SentenceTransformer's batch encoding
    embeddings = self.sentence_transformer.encode(
        texts,
        batch_size=len(texts),
        convert_to_numpy=True,
        show_progress_bar=False
    )
    
    # Returns: List of numpy arrays, each with 768 floats
    return embeddings
```

---

### What Happens Inside the AI Model?

```
Input Text: "Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: ÙŠØ³Ù…Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„"
              â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AI MODEL PROCESSING                     â”‚
â”‚  (paraphrase-multilingual-mpnet-base-v2)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: Tokenization
â”œâ”€ Split text into tokens (words/subwords)
â”œâ”€ Convert to token IDs
â”œâ”€ Add special tokens [CLS], [SEP]
â”‚
â”‚ "Ø§Ù„Ù…Ø§Ø¯Ø©" â†’ token_id: 1234
â”‚ "Ø§Ù„Ø£ÙˆÙ„Ù‰" â†’ token_id: 5678
â”‚ ...
â”‚
â”‚ Result: [101, 1234, 5678, ..., 102]
â”‚         â””â”€CLS token      SEP tokenâ”€â”˜

STEP 2: BERT Encoding (12 Transformer Layers)
â”œâ”€ Layer 1: Self-attention + Feed-forward
â”œâ”€ Layer 2: Self-attention + Feed-forward
â”œâ”€ ...
â”œâ”€ Layer 12: Self-attention + Feed-forward
â”‚
â”‚ Each layer:
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   â”‚ Multi-Head Self-Attention        â”‚
â”‚   â”‚ (captures relationships between  â”‚
â”‚   â”‚  words in the sentence)          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                  â”‚
â”‚                  â–¼
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   â”‚ Feed-Forward Neural Network      â”‚
â”‚   â”‚ (processes the attention output) â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                  â”‚
â”‚                  â–¼
â”‚   Hidden States: [batch, seq_len, 768]

STEP 3: Mean Pooling
â”œâ”€ Average all token embeddings
â”œâ”€ Reduces [seq_len, 768] â†’ [768]
â”‚
â”‚ Formula: mean(hidden_states, dim=1)
â”‚
â”‚ Result: [0.123, -0.456, 0.789, ..., 0.234]
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                    768 values

STEP 4: Normalization (Optional)
â”œâ”€ Normalize vector to unit length
â”œâ”€ Makes cosine similarity more meaningful
â”‚
â”‚ Formula: embedding / ||embedding||
â”‚
â”‚ Final Output: [0.0145, -0.0538, 0.0931, ..., 0.0276]
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                        768 normalized values
```

---

## ğŸ“Š Database Storage

### How Embeddings Are Stored:

```sql
-- Table: knowledge_chunk
CREATE TABLE knowledge_chunk (
    id INTEGER PRIMARY KEY,
    content TEXT,  -- Original text
    embedding_vector TEXT,  -- JSON array of 768 floats
    law_source_id INTEGER,
    ...
);

-- Example Record:
INSERT INTO knowledge_chunk VALUES (
    123,
    'Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: ÙŠØ³Ù…Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„',
    '[0.123, -0.456, 0.789, ..., 0.234]',  -- 768 floats as JSON
    5,
    ...
);
```

### Storage Format:

```python
# In Python
embedding = [0.123, -0.456, 0.789, ...]  # numpy array or list

# Convert to JSON string for SQLite
embedding_json = json.dumps(embedding)
# Result: '[0.123, -0.456, 0.789, ...]'

# Store in database
chunk.embedding_vector = embedding_json

# Later, retrieve from database
embedding_str = chunk.embedding_vector
embedding = json.loads(embedding_str)
# Result: [0.123, -0.456, 0.789, ...]
```

---

## ğŸŒ API Endpoints

### 1. Generate Embeddings for Document

```http
POST /api/v1/embeddings/documents/{document_id}/generate
Authorization: Bearer <JWT_TOKEN>
```

**Parameters**:
- `document_id`: ID of the document
- `overwrite`: (optional) Regenerate existing embeddings

**Example**:
```bash
curl -X POST "http://localhost:8000/api/v1/embeddings/documents/5/generate?overwrite=false" \
  -H "Authorization: Bearer eyJhbGc..."
```

**Response**:
```json
{
  "success": true,
  "message": "Generated embeddings for 45 chunks in document 5",
  "data": {
    "document_id": 5,
    "total_chunks": 45,
    "processed_chunks": 45,
    "failed_chunks": 0,
    "processing_time": "15.2s"
  },
  "errors": []
}
```

---

### 2. Generate Embeddings for Specific Chunks

```http
POST /api/v1/embeddings/chunks/batch-generate?chunk_ids=1,2,3,4,5
Authorization: Bearer <JWT_TOKEN>
```

**Parameters**:
- `chunk_ids`: List of chunk IDs (max 1000)
- `overwrite`: (optional) Regenerate existing embeddings

**Example**:
```bash
curl -X POST "http://localhost:8000/api/v1/embeddings/chunks/batch-generate?chunk_ids=1,2,3,4,5&overwrite=false" \
  -H "Authorization: Bearer eyJhbGc..."
```

---

### 3. Check Embedding Status

```http
GET /api/v1/embeddings/documents/{document_id}/status
Authorization: Bearer <JWT_TOKEN>
```

**Example**:
```bash
curl "http://localhost:8000/api/v1/embeddings/documents/5/status" \
  -H "Authorization: Bearer eyJhbGc..."
```

**Response**:
```json
{
  "success": true,
  "message": "Embedding status retrieved",
  "data": {
    "document_id": 5,
    "total_chunks": 45,
    "chunks_with_embeddings": 45,
    "chunks_without_embeddings": 0,
    "completion_percentage": 100.0,
    "status": "complete"
  },
  "errors": []
}
```

---

## ğŸ”§ Usage Examples

### Example 1: Generate Embeddings After Upload

```python
# After uploading a law PDF
import requests

# 1. Upload law
response = requests.post(
    "http://localhost:8000/api/v1/laws/upload",
    files={"pdf_file": open("law.pdf", "rb")},
    data={
        "law_name": "Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ",
        "law_type": "law"
    },
    headers={"Authorization": f"Bearer {token}"}
)

law_id = response.json()["data"]["law_source"]["id"]

# 2. Generate embeddings
response = requests.post(
    f"http://localhost:8000/api/v1/embeddings/documents/{law_id}/generate",
    headers={"Authorization": f"Bearer {token}"}
)

print(response.json())
# {
#   "success": true,
#   "message": "Generated embeddings for 45 chunks..."
# }
```

---

### Example 2: Batch Generate via Script

```bash
# Check current status
python scripts/generate_embeddings_batch.py --status

# Output:
# ğŸ“Š SYSTEM STATUS
# ğŸ“¦ Total chunks: 600
# âœ… With embeddings: 200
# â³ Without embeddings: 400
# ğŸ“ˆ Completion: 33.33%

# Generate embeddings for pending chunks
python scripts/generate_embeddings_batch.py --pending

# Output:
# ğŸš€ Starting PENDING chunks embedding generation
# ğŸ“¦ Found 400 pending chunks
# âš™ï¸  Processing batch 1/13
# âš™ï¸  Processing batch 2/13
# ...
# âœ… Batch processing complete: 395 successful, 5 failed
# â±ï¸  Duration: 45.3 seconds
# âš¡ Speed: 8.7 chunks/sec
```

---

### Example 3: Regenerate with New Model

```bash
# Update to Arabic BERT model
python scripts/regenerate_embeddings.py

# This will:
# 1. Find chunks without embeddings
# 2. Load Arabic BERT model
# 3. Generate embeddings
# 4. Verify results
# 5. Test search functionality
```

---

## ğŸ“ˆ Performance Metrics

### Processing Speed:

| Hardware | Speed (chunks/sec) | Time for 1000 chunks |
|----------|-------------------|---------------------|
| **CPU (Intel i7)** | 8-12 | ~90 seconds |
| **GPU (NVIDIA T4)** | 40-60 | ~20 seconds |
| **GPU (NVIDIA A100)** | 100-150 | ~7 seconds |

### Memory Usage:

| Model | RAM (CPU) | VRAM (GPU) |
|-------|-----------|------------|
| paraphrase-multilingual | ~2GB | ~1.5GB |
| arabert | ~2GB | ~1.5GB |
| OpenAI API | Minimal | N/A |

### Batch Size Impact:

| Batch Size | Speed | Memory |
|------------|-------|--------|
| 16 | Slower | Low |
| 32 | Balanced | Medium |
| 64 | **Optimal** | High |
| 128 | Fastest | Very High |

---

## âœ… Summary

### The Complete Process:

1. **Upload Document** â†’ PDF extracted and chunked
2. **Run Script** â†’ `python scripts/generate_embeddings_batch.py --pending`
3. **Load Model** â†’ AI model initialized (768-dimensional BERT)
4. **Process Batches** â†’ Chunks processed 64 at a time
5. **Generate Embeddings** â†’ Each text â†’ [768 numbers]
6. **Save to DB** â†’ Stored as JSON in `embedding_vector` column
7. **Enable Search** â†’ Now available for semantic search!

### Key Files:

- **Scripts**: `scripts/generate_embeddings_batch.py`, `scripts/regenerate_embeddings.py`
- **Service**: `app/services/arabic_legal_embedding_service.py`
- **API**: `app/routes/embedding_router.py`
- **Models**: Sentence Transformers (BERT-based)

### Default Model:

- **Name**: `paraphrase-multilingual-mpnet-base-v2`
- **Dimension**: 768 floats
- **Speed**: 8-12 chunks/sec (CPU), 40-60 (GPU)
- **Quality**: Excellent for Arabic + English

---

**Created**: October 9, 2025  
**Version**: 1.0  
**Status**: Production Ready âœ…

