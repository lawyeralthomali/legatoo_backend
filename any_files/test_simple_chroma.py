"""
Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¨Ø³Ø· Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙˆØ§Ø¯ Ù…Ø¹ Chroma
"""

import asyncio
import json
from app.db.database import AsyncSessionLocal
from app.models.legal_knowledge import KnowledgeDocument, LawSource, LawArticle, KnowledgeChunk
from app.services.document_parser_service import VectorstoreManager

async def test_simple_chroma():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¨Ø³Ø· Ù…Ø¹ Chroma"""
    
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¨Ø³Ø· Ù…Ø¹ Chroma...")
    
    try:
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù
        with open('data_set/files/saudi_labor_law.json', 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        print(f"âœ… ØªÙ… Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {len(json_data['law_sources']['articles'])} Ù…Ø§Ø¯Ø©")
        
        async with AsyncSessionLocal() as db:
            # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ«ÙŠÙ‚Ø©
            document = KnowledgeDocument(
                title="Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ",
                category="law",
                file_path="data_set/files/saudi_labor_law.json",
                file_hash="test_hash_123",
                source_type='uploaded',
                status='raw',
                uploaded_by=1
            )
            
            db.add(document)
            await db.commit()
            await db.refresh(document)
            
            print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©: {document.id}")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµØ¯Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ
            law_source_data = json_data['law_sources']
            law_source = LawSource(
                name=law_source_data['name'],
                type=law_source_data['type'],
                jurisdiction=law_source_data['jurisdiction'],
                issuing_authority=law_source_data['issuing_authority'],
                knowledge_document_id=document.id,
                status='processed'
            )
            
            db.add(law_source)
            await db.commit()
            await db.refresh(law_source)
            
            print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ: {law_source.id}")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙÙ‚Ø· Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
            articles_data = law_source_data['articles'][:3]  # Ø£ÙˆÙ„ 3 Ù…ÙˆØ§Ø¯ ÙÙ‚Ø·
            processed_count = 0
            
            print(f"ğŸ”„ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(articles_data)} Ù…Ø§Ø¯Ø©...")
            
            # Ø¥Ù†Ø´Ø§Ø¡ VectorstoreManager
            vectorstore_manager = VectorstoreManager()
            vectorstore = vectorstore_manager.get_vectorstore()
            
            for i, article_data in enumerate(articles_data):
                try:
                    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø§Ø¯Ø©
                    article = LawArticle(
                        law_source_id=law_source.id,
                        article_number=article_data.get('article'),
                        title=article_data.get('title'),
                        content=article_data['text'],
                        order_index=i,
                        source_document_id=document.id
                    )
                    
                    db.add(article)
                    await db.commit()
                    await db.refresh(article)
                    
                    processed_count += 1
                    print(f"ğŸ“„ ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø§Ø¯Ø© {i+1}: {article.article_number}")
                    
                    # Ø¥Ù†Ø´Ø§Ø¡ chunk Ø¨Ø³ÙŠØ·
                    try:
                        chunk = KnowledgeChunk(
                            document_id=document.id,
                            chunk_index=0,
                            content=article.content,
                            tokens_count=len(article.content.split()),
                            law_source_id=law_source.id,
                            article_id=article.id,
                            order_index=0,
                            verified_by_admin=False
                        )
                        
                        db.add(chunk)
                        await db.commit()
                        await db.refresh(chunk)
                        
                        print(f"ğŸ“¦ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ chunk {chunk.id} Ù„Ù„Ù…Ø§Ø¯Ø© {article.article_number}")
                        
                        # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Chroma
                        try:
                            metadata = {
                                "article": article.article_number,
                                "law_name": law_source.name,
                                "law_type": law_source.type,
                                "jurisdiction": law_source.jurisdiction,
                                "document_title": document.title
                            }
                            
                            vectorstore.add_texts(
                                texts=[article.content],
                                metadatas=[metadata],
                                ids=[str(chunk.id)]
                            )
                            
                            vectorstore.persist()
                            print(f"âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© chunk {chunk.id} Ø¥Ù„Ù‰ Chroma")
                            
                        except Exception as chroma_error:
                            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© chunk Ø¥Ù„Ù‰ Chroma: {chroma_error}")
                            
                    except Exception as chunk_error:
                        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ chunk: {chunk_error}")
                        
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø§Ø¯Ø© {i+1}: {e}")
                    continue
            
            print(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {processed_count} Ù…Ø§Ø¯Ø© Ø¨Ù†Ø¬Ø§Ø­")
            
            # ÙØ­Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            from sqlalchemy import select
            articles_result = await db.execute(select(LawArticle))
            articles = articles_result.scalars().all()
            
            chunks_result = await db.execute(select(KnowledgeChunk))
            chunks = chunks_result.scalars().all()
            
            print(f"\nğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
            print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ§Ø¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(articles)}")
            print(f"ğŸ“¦ Ø¹Ø¯Ø¯ Ø§Ù„Ù€ chunks ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(chunks)}")
            
            # ÙØ­Øµ Chroma
            try:
                collection = vectorstore._collection
                chroma_count = collection.count()
                print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ù€ chunks ÙÙŠ Chroma: {chroma_count}")
                
                if chroma_count > 0:
                    print(f"âœ… Chroma ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")
                    
                    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø«
                    try:
                        results = vectorstore.similarity_search("Ø¹Ù…Ù„", k=2)
                        print(f"ğŸ” Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«:")
                        for j, result in enumerate(results):
                            print(f"   {j+1}. {result.page_content[:50]}...")
                    except Exception as search_error:
                        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {search_error}")
                else:
                    print(f"âš ï¸ Chroma ÙØ§Ø±Øº!")
                    
            except Exception as chroma_error:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Chroma: {chroma_error}")
            
            print(f"\nğŸ‰ ØªÙ… Ø¥Ù†Ø¬Ø§Ø² Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙˆØ§Ø¯ Ù…Ø¹ Chroma!")
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_simple_chroma())
